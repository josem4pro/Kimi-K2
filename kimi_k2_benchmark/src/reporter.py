"""
Kimi K2 Benchmark Reporter
Report generation and visualization.
"""
import json
from pathlib import Path
from typing import Any

import matplotlib
matplotlib.use('Agg')  # Non-interactive backend
import matplotlib.pyplot as plt


def generate_markdown_report(metrics: dict[str, Any], out_path: Path) -> None:
    """
    Generate a comprehensive Markdown report.

    Args:
        metrics: Dictionary of metrics per model
        out_path: Path to output file
    """
    report = []

    # Title
    report.append("# Kimi K2 Benchmark Report\n")
    report.append("## Executive Summary\n")
    report.append("This report compares Kimi K2 (normal vs heavy mode) against Qwen3-Coder:30B.\n\n")

    # Key findings
    report.append("### Key Findings\n")

    # Find best model by accuracy
    best_model = max(metrics.keys(), key=lambda m: metrics[m].get("accuracy", 0))
    best_accuracy = metrics[best_model].get("accuracy", 0)
    report.append(f"- **Best Overall Accuracy**: {best_model} ({best_accuracy:.2f}%)\n")

    # Heavy mode advantage
    if "kimi_k2_normal" in metrics and "kimi_k2_heavy" in metrics:
        normal_acc = metrics["kimi_k2_normal"].get("accuracy", 0)
        heavy_acc = metrics["kimi_k2_heavy"].get("accuracy", 0)
        advantage = ((heavy_acc - normal_acc) / normal_acc * 100) if normal_acc > 0 else 0
        report.append(f"- **Heavy Mode Advantage**: {advantage:.2f}% improvement in accuracy\n")

    report.append("\n")

    # Comparison table
    report.append("## Model Comparison\n")
    report.append("| Model | Accuracy (%) | Mean Latency (s) | Tokens/s |\n")
    report.append("|-------|--------------|------------------|----------|\n")

    for model_id in sorted(metrics.keys()):
        m = metrics[model_id]
        acc = m.get("accuracy", 0)
        lat = m.get("mean_latency", 0)
        tps = m.get("mean_tokens_per_second", 0)
        report.append(f"| {model_id} | {acc:.2f} | {lat:.3f} | {tps:.1f} |\n")

    report.append("\n")

    # Heavy mode analysis
    report.append("## Heavy Mode Analysis\n")
    if "kimi_k2_heavy" in metrics:
        heavy_metrics = metrics["kimi_k2_heavy"]
        report.append("Heavy mode uses 8 parallel trajectories with hybridization.\n\n")

        if "heavy_mode_advantage" in heavy_metrics:
            adv = heavy_metrics["heavy_mode_advantage"]
            report.append(f"- **Overall Advantage**: {adv}%\n")

        if "trajectory_diversity" in heavy_metrics:
            div = heavy_metrics["trajectory_diversity"]
            report.append(f"- **Trajectory Diversity**: {div:.2f}\n")

    report.append("\n")

    # Recommendations
    report.append("## Recommendations\n")
    report.append("Based on the benchmark results:\n\n")

    # Generate recommendations based on data
    if "kimi_k2_heavy" in metrics and "kimi_k2_normal" in metrics:
        normal_acc = metrics["kimi_k2_normal"].get("accuracy", 0)
        heavy_acc = metrics["kimi_k2_heavy"].get("accuracy", 0)
        normal_lat = metrics["kimi_k2_normal"].get("mean_latency", 0)
        heavy_lat = metrics["kimi_k2_heavy"].get("mean_latency", 0)

        if heavy_acc > normal_acc + 5:
            report.append("1. **Use Heavy Mode** for complex reasoning tasks where accuracy is critical\n")
        else:
            report.append("1. **Use Normal Mode** for general tasks to save cost\n")

        if heavy_lat > normal_lat * 1.5:
            report.append("2. **Avoid Heavy Mode** for latency-sensitive applications\n")
        else:
            report.append("2. **Heavy Mode** has acceptable latency overhead\n")

    if "qwen3_coder_30b" in metrics:
        qwen_lat = metrics["qwen3_coder_30b"].get("mean_latency", 0)
        report.append(f"3. **Qwen3-Coder:30B** offers fast local inference ({qwen_lat:.2f}s mean)\n")

    report.append("\n")

    # Footer
    report.append("---\n")
    report.append("*Generated by Kimi K2 Benchmark Framework*\n")

    # Write to file
    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text("".join(report))


def generate_plots(metrics: dict[str, Any], out_dir: Path) -> None:
    """
    Generate visualization plots.

    Args:
        metrics: Dictionary of metrics per model
        out_dir: Directory to save plots
    """
    out_dir.mkdir(parents=True, exist_ok=True)

    # Generate accuracy comparison chart
    generate_accuracy_chart(metrics, out_dir / "accuracy_comparison.png")


def generate_accuracy_chart(metrics: dict[str, Any], out_path: Path) -> None:
    """
    Generate accuracy comparison bar chart.

    Args:
        metrics: Dictionary of metrics per model
        out_path: Path to save chart
    """
    models = list(metrics.keys())
    accuracies = [metrics[m].get("accuracy", 0) for m in models]

    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(models, accuracies, color=['blue', 'green', 'orange'][:len(models)])

    ax.set_xlabel('Model')
    ax.set_ylabel('Accuracy (%)')
    ax.set_title('Model Accuracy Comparison')
    ax.set_ylim(0, 100)

    # Add value labels on bars
    for bar, acc in zip(bars, accuracies):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                f'{acc:.1f}%', ha='center', va='bottom')

    plt.tight_layout()
    plt.savefig(out_path, dpi=150)
    plt.close()


def generate_latency_chart(metrics: dict[str, Any], out_path: Path) -> None:
    """
    Generate latency comparison bar chart.

    Args:
        metrics: Dictionary of metrics per model
        out_path: Path to save chart
    """
    models = list(metrics.keys())
    latencies = [metrics[m].get("mean_latency", 0) for m in models]

    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(models, latencies, color=['blue', 'green', 'orange'][:len(models)])

    ax.set_xlabel('Model')
    ax.set_ylabel('Mean Latency (seconds)')
    ax.set_title('Model Latency Comparison')

    # Add value labels
    for bar, lat in zip(bars, latencies):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                f'{lat:.2f}s', ha='center', va='bottom')

    plt.tight_layout()
    plt.savefig(out_path, dpi=150)
    plt.close()


def generate_heavy_mode_chart(
    category_advantages: dict[str, float],
    out_path: Path
) -> None:
    """
    Generate chart showing heavy mode advantage by category.

    Args:
        category_advantages: Dict mapping category to advantage percentage
        out_path: Path to save chart
    """
    categories = list(category_advantages.keys())
    advantages = list(category_advantages.values())

    fig, ax = plt.subplots(figsize=(12, 6))
    bars = ax.bar(categories, advantages, color='purple')

    ax.set_xlabel('Benchmark Category')
    ax.set_ylabel('Heavy Mode Advantage (%)')
    ax.set_title('Heavy Mode Performance Gain by Category')
    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)

    # Add value labels
    for bar, adv in zip(bars, advantages):
        y_pos = bar.get_height() + (0.5 if adv >= 0 else -1.5)
        ax.text(bar.get_x() + bar.get_width()/2, y_pos,
                f'{adv:.1f}%', ha='center', va='bottom')

    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(out_path, dpi=150)
    plt.close()


def export_to_csv(metrics: dict[str, Any], out_path: Path) -> None:
    """
    Export metrics to CSV file.

    Args:
        metrics: Dictionary of metrics per model
        out_path: Path to output CSV
    """
    import csv

    out_path.parent.mkdir(parents=True, exist_ok=True)

    # Flatten metrics for CSV
    rows = []
    for model_id, model_metrics in metrics.items():
        row = {"model": model_id}
        for key, value in model_metrics.items():
            if not isinstance(value, dict):  # Skip nested dicts
                row[key] = value
        rows.append(row)

    # Get all keys for header
    all_keys = set()
    for row in rows:
        all_keys.update(row.keys())
    headers = sorted(all_keys)

    with open(out_path, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=headers)
        writer.writeheader()
        writer.writerows(rows)


def export_to_json(metrics: dict[str, Any], out_path: Path) -> None:
    """
    Export metrics to JSON file.

    Args:
        metrics: Dictionary of metrics
        out_path: Path to output JSON
    """
    out_path.parent.mkdir(parents=True, exist_ok=True)

    with open(out_path, 'w') as f:
        json.dump(metrics, f, indent=2, default=str)


def generate_decision_tree(metrics: dict[str, Any]) -> dict[str, Any]:
    """
    Generate a decision tree for when to use each model.

    Args:
        metrics: Dictionary of metrics per model

    Returns:
        Decision tree structure
    """
    tree = {
        "question": "What is your primary concern?",
        "branches": []
    }

    # Accuracy-focused branch
    accuracy_branch = {
        "answer": "Maximum accuracy",
        "recommendation": None
    }

    # Find best by accuracy
    if "kimi_k2_heavy" in metrics:
        heavy_acc = metrics.get("kimi_k2_heavy", {}).get("accuracy", 0)
        normal_acc = metrics.get("kimi_k2_normal", {}).get("accuracy", 0)

        if heavy_acc > normal_acc + 5:
            accuracy_branch["recommendation"] = "Use Kimi K2 Heavy Mode"
        else:
            accuracy_branch["recommendation"] = "Use Kimi K2 Normal Mode (similar accuracy, lower cost)"
    else:
        accuracy_branch["recommendation"] = "Use Kimi K2 Normal Mode"

    tree["branches"].append(accuracy_branch)

    # Speed-focused branch
    speed_branch = {
        "answer": "Low latency",
        "recommendation": None
    }

    if "qwen3_coder_30b" in metrics:
        qwen_lat = metrics.get("qwen3_coder_30b", {}).get("mean_latency", 999)
        kimi_lat = metrics.get("kimi_k2_normal", {}).get("mean_latency", 999)

        if qwen_lat < kimi_lat:
            speed_branch["recommendation"] = "Use Qwen3-Coder:30B (local, fastest)"
        else:
            speed_branch["recommendation"] = "Use Kimi K2 Normal Mode"
    else:
        speed_branch["recommendation"] = "Use Kimi K2 Normal Mode"

    tree["branches"].append(speed_branch)

    # Cost-focused branch
    cost_branch = {
        "answer": "Minimize cost",
        "recommendation": "Use Qwen3-Coder:30B (free local) or Kimi K2 Normal Mode"
    }
    tree["branches"].append(cost_branch)

    return tree


def generate_recommendations(metrics: dict[str, Any]) -> list[str]:
    """
    Generate list of practical recommendations.

    Args:
        metrics: Dictionary of metrics per model

    Returns:
        List of recommendation strings
    """
    recommendations = []

    # Analyze metrics
    models = list(metrics.keys())

    if not models:
        return ["No metrics available for recommendations."]

    # Best accuracy
    best_acc_model = max(models, key=lambda m: metrics[m].get("accuracy", 0))
    best_acc = metrics[best_acc_model].get("accuracy", 0)
    recommendations.append(
        f"For highest accuracy, use {best_acc_model} ({best_acc:.1f}% accuracy)"
    )

    # Best latency
    best_lat_model = min(models, key=lambda m: metrics[m].get("mean_latency", 999))
    best_lat = metrics[best_lat_model].get("mean_latency", 0)
    recommendations.append(
        f"For lowest latency, use {best_lat_model} ({best_lat:.2f}s mean)"
    )

    # Heavy mode specific
    if "kimi_k2_heavy" in metrics and "kimi_k2_normal" in metrics:
        heavy_acc = metrics["kimi_k2_heavy"].get("accuracy", 0)
        normal_acc = metrics["kimi_k2_normal"].get("accuracy", 0)
        heavy_lat = metrics["kimi_k2_heavy"].get("mean_latency", 0)
        normal_lat = metrics["kimi_k2_normal"].get("mean_latency", 0)

        if heavy_acc > normal_acc:
            advantage = ((heavy_acc - normal_acc) / normal_acc * 100) if normal_acc > 0 else 0
            recommendations.append(
                f"Heavy mode provides {advantage:.1f}% accuracy improvement over normal mode"
            )
        else:
            recommendations.append(
                "Heavy mode does not significantly improve accuracy; prefer normal mode"
            )

        if heavy_lat > normal_lat * 1.5:
            recommendations.append(
                f"Heavy mode is {(heavy_lat/normal_lat):.1f}x slower than normal mode"
            )

    # Qwen specific
    if "qwen3_coder_30b" in metrics:
        recommendations.append(
            "Qwen3-Coder:30B runs locally with no API costs"
        )

    return recommendations
